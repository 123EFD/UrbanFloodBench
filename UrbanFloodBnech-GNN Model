{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14761150,"sourceType":"datasetVersion","datasetId":9434919}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# --- INSTALLATION BLOCK ---\nimport torch\n\ndef install_pyg():\n    print(f\"PyTorch Version: {torch.__version__}\")\n    print(f\"CUDA Version: {torch.version.cuda}\")\n    \n    # 1. Install Core PyG\n    !pip install -q torch-geometric\n    \n    # 2. Install Optional Dependencies (Scatter & Sparse are needed for GNN layers)\n    # We use a helper to find the right wheel for the installed Torch/CUDA version\n    try:\n        import torch_scatter, torch_sparse\n    except ImportError:\n        print(\"Installing dependencies (Scatter, Sparse)... this may take a minute.\")\n        # General install command that looks for binary wheels\n        !pip install -q torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n\ninstall_pyg()\nprint(\"âœ… PyTorch Geometric Installed! You can now run the Graph Construction code.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-08T10:00:44.878054Z","iopub.execute_input":"2026-02-08T10:00:44.878292Z","iopub.status.idle":"2026-02-08T10:01:02.372067Z","shell.execute_reply.started":"2026-02-08T10:00:44.878271Z","shell.execute_reply":"2026-02-08T10:01:02.371162Z"}},"outputs":[{"name":"stdout","text":"PyTorch Version: 2.8.0+cu126\nCUDA Version: 12.6\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling dependencies (Scatter, Sparse)... this may take a minute.\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m108.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hâœ… PyTorch Geometric Installed! You can now run the Graph Construction code.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-08T10:01:02.373562Z","iopub.execute_input":"2026-02-08T10:01:02.374137Z","iopub.status.idle":"2026-02-08T10:01:02.698122Z","shell.execute_reply.started":"2026-02-08T10:01:02.374076Z","shell.execute_reply":"2026-02-08T10:01:02.697488Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/submission/submission_gnn.csv\n/kaggle/input/submission/submission.csv\n/kaggle/input/urbanfloodbench/test_2d_edges_dynamic_all.csv\n/kaggle/input/urbanfloodbench/test_2d_edges_static.csv\n/kaggle/input/urbanfloodbench/timesteps.csv\n/kaggle/input/urbanfloodbench/sample_submission.csv\n/kaggle/input/urbanfloodbench/test_2d_nodes_dynamic_all.csv\n/kaggle/input/urbanfloodbench/test_1d_edges_dynamic_all.csv\n/kaggle/input/urbanfloodbench/1d2d_connections.csv\n/kaggle/input/urbanfloodbench/1d_nodes_static.csv\n/kaggle/input/urbanfloodbench/2d_nodes_dynamic_all.csv\n/kaggle/input/urbanfloodbench/2d_edges_dynamic_all.csv\n/kaggle/input/urbanfloodbench/test_timesteps.csv\n/kaggle/input/urbanfloodbench/1d_edge_index.csv\n/kaggle/input/urbanfloodbench/2d_edges_static.csv\n/kaggle/input/urbanfloodbench/test_2d_edge_index.csv\n/kaggle/input/urbanfloodbench/1d_edges_static.csv\n/kaggle/input/urbanfloodbench/test_1d_edges_static.csv\n/kaggle/input/urbanfloodbench/1d_edges_dynamic_all.csv\n/kaggle/input/urbanfloodbench/sample_submission.parquet\n/kaggle/input/urbanfloodbench/submission.csv\n/kaggle/input/urbanfloodbench/2d_edge_index.csv\n/kaggle/input/urbanfloodbench/2d_nodes_static.csv\n/kaggle/input/urbanfloodbench/test_2d_nodes_static.csv\n/kaggle/input/urbanfloodbench/test_1d_nodes_static.csv\n/kaggle/input/urbanfloodbench/test_1d2d_connections.csv\n/kaggle/input/urbanfloodbench/1d_nodes_dynamic_all.csv\n/kaggle/input/urbanfloodbench/test_1d_nodes_dynamic_all.csv\n/kaggle/input/urbanfloodbench/my_raw_predictions.csv\n/kaggle/input/urbanfloodbench/test_1d_edge_index.csv\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n# Load Training Data\nprint(\"Loading training data for statistics...\")\ntrain_1d = pd.read_csv('/kaggle/input/urbanfloodbench/1d_nodes_dynamic_all.csv')\ntrain_2d = pd.read_csv('/kaggle/input/urbanfloodbench/2d_nodes_dynamic_all.csv')\n\n# Calculate Sigma (Standard Deviation)\nsigma_1d = train_1d['water_level'].std()\nsigma_2d = train_2d['water_level'].std()\n\nprint(f\"âœ… Calculated Constants:\")\nprint(f\"Sigma 1D: {sigma_1d}\")\nprint(f\"Sigma 2D: {sigma_2d}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-08T10:01:02.699652Z","iopub.execute_input":"2026-02-08T10:01:02.699949Z","iopub.status.idle":"2026-02-08T10:01:03.090693Z","shell.execute_reply.started":"2026-02-08T10:01:02.699930Z","shell.execute_reply":"2026-02-08T10:01:03.090105Z"}},"outputs":[{"name":"stdout","text":"Loading training data for statistics...\nâœ… Calculated Constants:\nSigma 1D: 16.800592891244573\nSigma 2D: 14.362006861860646\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass StandardizedRMSELoss(nn.Module):\n    def __init__(self, sigma_1d, sigma_2d):\n        super().__init__()\n        # Register sigmas as buffers so they move to GPU automatically with the model\n        self.register_buffer('sigma_1d', torch.tensor(sigma_1d))\n        self.register_buffer('sigma_2d', torch.tensor(sigma_2d))\n        self.mse = nn.MSELoss()\n\n    def forward(self, pred, target, node_type_mask):\n        \"\"\"\n        pred: Predicted values [Batch, Nodes]\n        target: Actual values [Batch, Nodes]\n        node_type_mask: Boolean tensor where True = 1D Node, False = 2D Node\n        \"\"\"\n        # Split predictions by node type\n        pred_1d = pred[node_type_mask]\n        target_1d = target[node_type_mask]\n        \n        pred_2d = pred[~node_type_mask]\n        target_2d = target[~node_type_mask]\n        \n        # Calculate RMSE for each\n        rmse_1d = torch.sqrt(self.mse(pred_1d, target_1d))\n        rmse_2d = torch.sqrt(self.mse(pred_2d, target_2d))\n        \n        # Standardize\n        std_rmse_1d = rmse_1d / self.sigma_1d\n        std_rmse_2d = rmse_2d / self.sigma_2d\n        \n        # Return average (Competition Metric)\n        return (std_rmse_1d + std_rmse_2d) / 2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-08T10:01:03.091601Z","iopub.execute_input":"2026-02-08T10:01:03.091896Z","iopub.status.idle":"2026-02-08T10:01:03.098016Z","shell.execute_reply.started":"2026-02-08T10:01:03.091863Z","shell.execute_reply":"2026-02-08T10:01:03.097467Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import pandas as pd\n\n# Load the CSV file\nstatic_1d = pd.read_csv('/kaggle/input/urbanfloodbench/1d_nodes_static.csv')\nstatic_2d = pd.read_csv('/kaggle/input/urbanfloodbench/2d_nodes_static.csv')\n\n# Print the list of columns\nprint(static_1d.columns.tolist())\nprint(static_2d.columns.tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-08T10:01:03.098735Z","iopub.execute_input":"2026-02-08T10:01:03.098939Z","iopub.status.idle":"2026-02-08T10:01:03.133288Z","shell.execute_reply.started":"2026-02-08T10:01:03.098920Z","shell.execute_reply":"2026-02-08T10:01:03.132576Z"}},"outputs":[{"name":"stdout","text":"['node_idx', 'position_x', 'position_y', 'depth', 'invert_elevation', 'surface_elevation', 'base_area']\n['node_idx', 'position_x', 'position_y', 'area', 'roughness', 'min_elevation', 'elevation', 'aspect', 'curvature', 'flow_accumulation']\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"def build_unified_graph():\n    print(\"ğŸ—ï¸ Building Enhanced Unified Graph Object...\")\n    \n    # 1. Load Files\n    static_1d = pd.read_csv('/kaggle/input/urbanfloodbench/1d_nodes_static.csv')\n    static_2d = pd.read_csv('/kaggle/input/urbanfloodbench/2d_nodes_static.csv')\n    edges_1d_df = pd.read_csv('/kaggle/input/urbanfloodbench/1d_edge_index.csv')\n    edges_2d_df = pd.read_csv('/kaggle/input/urbanfloodbench/2d_edge_index.csv')\n    conn_df = pd.read_csv('/kaggle/input/urbanfloodbench/1d2d_connections.csv')\n    \n    num_1d_nodes = len(static_1d)\n    num_2d_nodes = len(static_2d)\n\n    # 2. Node Features (X)\n    feat_1d = static_1d[['position_x', 'position_y', 'invert_elevation', 'base_area']].values\n    feat_2d = static_2d[['position_x', 'position_y', 'elevation', 'area']].values\n    \n    combined_feats = np.vstack([feat_1d, feat_2d])\n    mean = combined_feats.mean(axis=0)\n    std = combined_feats.std(axis=0) + 1e-6\n    x_norm = (combined_feats - mean) / std\n    \n    x = torch.tensor(x_norm, dtype=torch.float)\n    type_mask = torch.cat([torch.zeros(num_1d_nodes), torch.ones(num_2d_nodes)]).reshape(-1, 1)\n    x = torch.cat([x, type_mask], dim=1)\n\n    # 3. Edge Indices\n    # 1D\n    src_1d = edges_1d_df['from_node'].values\n    dst_1d = edges_1d_df['to_node'].values\n    \n    # 2D (Shifted)\n    src_2d = edges_2d_df['from_node'].values + num_1d_nodes\n    dst_2d = edges_2d_df['to_node'].values + num_1d_nodes\n    \n    # Coupled (Shifted)\n    conn_df.columns = [c.strip() for c in conn_df.columns]\n    col_1d = next((c for c in conn_df.columns if '1d' in c.lower()), '1d_node_idx')\n    col_2d = next((c for c in conn_df.columns if '2d' in c.lower()), '2d_node_idx')\n    \n    src_c1 = conn_df[col_2d].values + num_1d_nodes \n    dst_c1 = conn_df[col_1d].values\n    src_c2 = conn_df[col_1d].values\n    dst_c2 = conn_df[col_2d].values + num_1d_nodes\n\n    all_src = np.concatenate([src_1d, src_2d, src_c1, src_c2])\n    all_dst = np.concatenate([dst_1d, dst_2d, dst_c1, dst_c2])\n    edge_index = torch.tensor([all_src, all_dst], dtype=torch.long)\n\n    # 4. Edge Weights (The \"Enhanced\" Part)\n    if 'length' in edges_1d_df.columns:\n        # Inverse length: closer nodes have stronger influence\n        weights_1d = 1.0 / (edges_1d_df['length'].values + 1e-6) \n    else:\n        weights_1d = np.ones(len(edges_1d_df))\n\n    weights_2d = np.ones(len(src_2d)) \n    weights_coupled = np.ones(len(src_c1) + len(src_c2))\n    \n    edge_weight = torch.tensor(np.concatenate([weights_1d, weights_2d, weights_coupled]), dtype=torch.float)\n\n    # 5. Create Object\n    data = Data(x=x, edge_index=edge_index, edge_attr=edge_weight)\n    data.num_1d = num_1d_nodes\n    data.num_2d = num_2d_nodes\n    \n    print(f\"âœ… Enhanced Graph Created!\")\n    print(f\"   Nodes: {num_1d_nodes + num_2d_nodes} | Edges: {edge_index.shape[1]}\")\n    \n    return data, mean, std\n\n# Execute\ngraph_data, feat_mean, feat_std = build_unified_graph()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-08T10:01:03.134193Z","iopub.execute_input":"2026-02-08T10:01:03.134535Z","iopub.status.idle":"2026-02-08T10:01:03.303020Z","shell.execute_reply.started":"2026-02-08T10:01:03.134504Z","shell.execute_reply":"2026-02-08T10:01:03.301738Z"}},"outputs":[{"name":"stdout","text":"ğŸ—ï¸ Building Enhanced Unified Graph Object...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_55/2803808386.py:48: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)\n  edge_index = torch.tensor([all_src, all_dst], dtype=torch.long)\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/2803808386.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;31m# Execute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m \u001b[0mgraph_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeat_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeat_std\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_unified_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_55/2803808386.py\u001b[0m in \u001b[0;36mbuild_unified_graph\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m# 5. Create Object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_attr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0medge_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_1d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_1d_nodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_2d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_2d_nodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'Data' is not defined"],"ename":"NameError","evalue":"name 'Data' is not defined","output_type":"error"}],"execution_count":6},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n\n# ==========================================\n# 1. TEMPORAL DATA PROCESSOR\n# ==========================================\ndef process_dynamic_data(graph_data):\n    print(\"â³ Processing Dynamic Signals...\")\n    \n    # Load raw dynamic files\n    # Note: We use TRAINING data here to build the training set\n    d1 = pd.read_csv('/kaggle/input/urbanfloodbench/1d_nodes_dynamic_all.csv')\n    d2 = pd.read_csv('/kaggle/input/urbanfloodbench/2d_nodes_dynamic_all.csv')\n    \n    # --- A. Pivot to Matrix Form [Timestep x Node] ---\n    # We need to guarantee the order matches the Graph Node IDs\n    # 1D Nodes: 0 to num_1d-1\n    # 2D Nodes: num_1d to num_1d+num_2d-1\n    \n    # 1. Pivot Water Levels\n    # We use 'sum' aggregation just in case of duplicates, though there shouldn't be any\n    w1 = d1.pivot_table(index='timestep', columns='node_idx', values='water_level', aggfunc='sum')\n    w2 = d2.pivot_table(index='timestep', columns='node_idx', values='water_level', aggfunc='sum')\n    \n    # 2. Pivot Forcing (Inlet Flow for 1D, Rainfall for 2D)\n    # Note: 2D usually has 'rainfall', 1D has 'inlet_flow'. \n    # We will combine them into a generic \"Forcing\" feature.\n    f1 = d1.pivot_table(index='timestep', columns='node_idx', values='inlet_flow', aggfunc='sum').fillna(0)\n    \n    # For 2D, if 'rainfall' column exists use it, else 0 (some datasets split rainfall separately)\n    if 'rainfall' in d2.columns:\n        f2 = d2.pivot_table(index='timestep', columns='node_idx', values='rainfall', aggfunc='sum').fillna(0)\n    else:\n        f2 = pd.DataFrame(0, index=w2.index, columns=w2.columns)\n\n    # --- B. Align & Concatenate ---\n    # Ensure all timesteps are present and sorted\n    common_index = w1.index.intersection(w2.index).sort_values()\n    w1, w2 = w1.loc[common_index], w2.loc[common_index]\n    f1, f2 = f1.loc[common_index], f2.loc[common_index]\n    \n    # Stack 1D and 2D horizontally to match Graph Node Order\n    # Water Matrix: [Time, Total_Nodes]\n    water_matrix = np.hstack([w1.values, w2.values])\n    \n    # Forcing Matrix: [Time, Total_Nodes]\n    forcing_matrix = np.hstack([f1.values, f2.values])\n    \n    # --- C. Create Feature Tensor ---\n    # Shape: [Time, Nodes, Features] -> Features = (Water_Level, Forcing)\n    # We stack them along the last dimension\n    # Feature 0: Water Level (Target)\n    # Feature 1: Forcing (Input only)\n    dynamic_tensor = np.stack([water_matrix, forcing_matrix], axis=-1)\n    \n    # Handle NaNs (Interpolate or Fill 0)\n    dynamic_tensor = np.nan_to_num(dynamic_tensor)\n    \n    print(f\"âœ… Dynamic Tensor Created!\")\n    print(f\"   Shape: {dynamic_tensor.shape} (Timesteps, Nodes, Features)\")\n    \n    return torch.FloatTensor(dynamic_tensor)\n\n# Execute\nfull_history = process_dynamic_data(graph_data)\n\n# ==========================================\n# 2. SLIDING WINDOW DATASET\n# ==========================================\nclass FloodDataset(Dataset):\n    def __init__(self, data_tensor, input_window=12, output_window=12):\n        \"\"\"\n        data_tensor: [Time, Nodes, Feats]\n        \"\"\"\n        self.data = data_tensor\n        self.input_window = input_window\n        self.output_window = output_window\n        \n    def __len__(self):\n        # We need enough history for input AND enough future for output\n        return len(self.data) - self.input_window - self.output_window\n    \n    def __getitem__(self, idx):\n        # Window: [t : t+In]\n        # Target: [t+In : t+In+Out]\n        \n        # X: Use both features (Water + Forcing)\n        x = self.data[idx : idx+self.input_window, :, :] \n        \n        # Y: Predict only Water Level (Feature 0)\n        # We generally predict the CHANGE or the absolute value. Let's predict Absolute.\n        y = self.data[idx+self.input_window : idx+self.input_window+self.output_window, :, 0]\n        \n        return x, y\n\n# Instantiate\ntrain_dataset = FloodDataset(full_history, input_window=12, output_window=12)\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n\nprint(f\"âœ… Data Loader Ready! Batches: {len(train_loader)}\")\n# Check shape of one batch\nsample_x, sample_y = next(iter(train_loader))\nprint(f\"   Sample X: {sample_x.shape} (Batch, In_Time, Nodes, Feats)\")\nprint(f\"   Sample Y: {sample_y.shape} (Batch, Out_Time, Nodes)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-08T10:01:03.303496Z","iopub.status.idle":"2026-02-08T10:01:03.303727Z","shell.execute_reply.started":"2026-02-08T10:01:03.303615Z","shell.execute_reply":"2026-02-08T10:01:03.303627Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv\n\nclass FloodSTGNN(nn.Module):\n    def __init__(self, num_nodes, in_channels=2, out_channels=12, hidden_dim=128):\n        super(FloodSTGNN, self).__init__()\n        \n        self.num_nodes = num_nodes\n        \n        # --- 1. FEATURE EMBEDDING (The Missing Link) ---\n        self.feature_embed = nn.Linear(in_channels, hidden_dim)\n\n        # --- 2. TEMPORAL BLOCK (LSTM) ---\n        self.lstm = nn.LSTM(input_size=hidden_dim, hidden_size=hidden_dim, \n                            num_layers=2, batch_first=True,dropout=0.2)\n        \n        # --- 3. SPATIAL BLOCK (Graph Convolution) ---\n        self.gcn1 = GCNConv(hidden_dim, hidden_dim)\n        self.gcn2 = GCNConv(hidden_dim, hidden_dim)\n        self.gcn3 = GCNConv(hidden_dim, hidden_dim)\n        \n        # --- 4. OUTPUT HEAD ---\n        self.fc = nn.Linear(hidden_dim, out_channels)\n\n    def forward(self, x, edge_index, edge_weight=None):\n        \"\"\"\n        x: [Batch, Time, Nodes, Features]\n        \"\"\"\n        B, T, N, F_in = x.shape\n        \n        # 1. Reshape for LSTM: [Batch*Nodes, Time, Features]\n        # We treat every node as an independent sequence initially\n        x_flat = x.permute(0, 2, 1, 3).reshape(B*N, T, F_in)\n        \n        # 2. Apply Embedding (Fixes the dimension mismatch)\n        # Shape: [B*N, T, 2] -> [B*N, T, 64]\n        x_flat = x.permute(0,2,1,3).reshape(B*N, T, F_in)\n        x_embedded = F.relu(self.feature_embed(x_flat))\n        x_embedded = F.relu(x_embedded) # Optional activation\n        \n        # 3. Run LSTM\n        lstm_out, _ = self.lstm(x_embedded)\n        \n        # Take the state at the last time step\n        last_hidden = lstm_out[:, -1, :] \n        \n        # 4. Reshape for GCN: [Batch, Nodes, Hidden]\n        gcn_in = last_hidden.view(B, N, -1)\n        \n        # 5. Run GCN (Spatial Mixing)\n        # We loop over the batch because GCN expects [Nodes, Feats]\n        gcn_outs = []\n        for b in range(B):\n            h = F.relu(self.gcn1(gcn_in[b], edge_index, edge_weight))\n            h = F.relu(self.gcn2(h, edge_index, edge_weight))\n            h = F.relu(self.gcn3(h, edge_index, edge_weight))\n            gcn_outs.append(h)\n            \n        # Stack back: [Batch, Nodes, Hidden]\n        gcn_final = torch.stack(gcn_outs)\n        \n        # 6. Final Prediction\n        # Shape: [Batch, Nodes, Out_Steps]\n        out = self.fc(gcn_final)\n        \n        return out\n\n# Re-instantiate the model on GPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = FloodSTGNN(num_nodes=3733, hidden_dim=128).to(device)\n\nprint(\"âœ… Corrected Model Defined!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-08T10:01:03.305138Z","iopub.status.idle":"2026-02-08T10:01:03.305455Z","shell.execute_reply.started":"2026-02-08T10:01:03.305280Z","shell.execute_reply":"2026-02-08T10:01:03.305304Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load both\ndf_xgb = pd.read_csv('/kaggle/input/submission/submission.csv')\ndf_gnn = pd.read_csv('/kaggle/input/submission/submission_gnn.csv')\n\n# Ensemble Weighting\n# The GNN is likely better at peaks, XGBoost is likely better at low-flow stability\ndf_xgb['water_level'] = (0.7 * df_gnn['water_level']) + (0.3 * df_xgb['water_level'])\n\ndf_xgb.to_csv('submission_ensemble.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-08T10:01:03.306268Z","iopub.status.idle":"2026-02-08T10:01:03.306606Z","shell.execute_reply.started":"2026-02-08T10:01:03.306441Z","shell.execute_reply":"2026-02-08T10:01:03.306461Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.optim as optim\n\n# ==========================================\n# 1. SETUP (Optimizer & Loss)\n# ==========================================\n# Hyperparameters\nLEARNING_RATE = 0.001\nEPOCHS = 10  # Start small to verify it works, then increase to 50+\n\n# Define Optimizer\noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\n# Re-define Loss (in case variables were lost)\n# Using the sigma values you calculated earlier:\nSIGMA_1D = 16.80\nSIGMA_2D = 14.36\n\nclass StandardizedRMSELoss(nn.Module):\n    def __init__(self, sigma_1d, sigma_2d):\n        super().__init__()\n        self.sigma_1d = sigma_1d\n        self.sigma_2d = sigma_2d\n        self.mse = nn.MSELoss()\n\n    def forward(self, pred, target, node_type_mask):\n        \"\"\"\n        Calculates Competition Metric: Average of (RMSE_1D / Sigma_1D) and (RMSE_2D / Sigma_2D)\n        \"\"\"\n        # Create masks based on the node_type_mask stored in the graph\n        # mask = 0 (1D), mask = 1 (2D)\n        mask_1d = (node_type_mask == 0).squeeze()\n        mask_2d = (node_type_mask == 1).squeeze()\n        \n        # Filter 1D Nodes\n        # Pred shape: [Batch, Time, Nodes] -> Select Nodes -> [Batch, Time, N_1D]\n        p1 = pred[:, :, mask_1d]\n        t1 = target[:, :, mask_1d]\n        rmse_1d = torch.sqrt(self.mse(p1, t1))\n        \n        # Filter 2D Nodes\n        p2 = pred[:, :, mask_2d]\n        t2 = target[:, :, mask_2d]\n        rmse_2d = torch.sqrt(self.mse(p2, t2))\n        \n        # Standardize and Average\n        loss = ( (rmse_1d / self.sigma_1d) + (rmse_2d / self.sigma_2d) ) / 2\n        return loss\n\ncriterion = StandardizedRMSELoss(SIGMA_1D, SIGMA_2D)\n\n# ==========================================\n# 2. TRAINING LOOP\n# ==========================================\nprint(\"ğŸš€ Starting ST-GNN Training...\")\nmodel.train() # Set to training mode\n\n# Get Graph Structure (Static Edges & Type Mask)\n# Move to GPU\nedge_index = graph_data.edge_index.to(device)\n# The last feature in x is the type mask (0 or 1)\nnode_type_mask = graph_data.x[:, -1].long().to(device) \n\nfor epoch in range(EPOCHS):\n    total_loss = 0\n    \n    for batch_idx, (x_batch, y_batch) in enumerate(train_loader):\n        # Move data to GPU\n        x_batch = x_batch.to(device) # [Batch, 12, Nodes, 2]\n        y_batch = y_batch.to(device) # [Batch, 12, Nodes]\n        \n        # 1. Zero Gradients\n        optimizer.zero_grad()\n        \n        # 2. Forward Pass\n        # Output: [Batch, Nodes, 12]\n        out = model(x_batch, edge_index)\n        \n        # 3. Shape Correction\n        # Target is [Batch, 12, Nodes] (Time in middle)\n        # Model Out is [Batch, Nodes, 12] (Time at end)\n        # We permute Out to match Target: [B, N, T] -> [B, T, N]\n        out = out.permute(0, 2, 1)\n        \n        # 4. Calculate Loss\n        loss = criterion(out, y_batch, node_type_mask)\n        \n        # 5. Backprop\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n        \n    avg_loss = total_loss / len(train_loader)\n    print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss (Std RMSE): {avg_loss:.4f}\")\n\nprint(\"âœ… Training Complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-08T10:01:03.307853Z","iopub.status.idle":"2026-02-08T10:01:03.308081Z","shell.execute_reply.started":"2026-02-08T10:01:03.307972Z","shell.execute_reply":"2026-02-08T10:01:03.307984Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport gc\nfrom torch.utils.data import DataLoader\nimport torch.optim as optim\n\n# ==========================================\n# 1. CLEANUP MEMORY\n# ==========================================\n# Delete old variables that might be on GPU\nif 'out' in locals(): del out\nif 'loss' in locals(): del loss\nif 'x_batch' in locals(): del x_batch\nif 'y_batch' in locals(): del y_batch\n\n# Force Garbage Collection\ngc.collect()\n# Clear PyTorch GPU Cache\ntorch.cuda.empty_cache()\nprint(\"ğŸ§¹ GPU Memory Cleared.\")\n\n# ==========================================\n# 2. REDUCE BATCH SIZE\n# ==========================================\n# We reduce batch size from 32 to 4 to fit in memory\n# This means it updates weights more often, which is fine.\nBATCH_SIZE = 4 \n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nprint(f\"ğŸ“‰ Batch Size reduced to {BATCH_SIZE}. Batches per epoch: {len(train_loader)}\")\n\n# ==========================================\n# 3. RE-RUN TRAINING LOOP\n# ==========================================\n# Re-initialize optimizer to clear old states\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nprint(\"ğŸš€ Restarting ST-GNN Training (Safe Mode)...\")\nmodel.train() \n\n# Ensure graph components are on GPU\nedge_index = graph_data.edge_index.to(device)\nnode_type_mask = graph_data.x[:, -1].long().to(device)\n\nfor epoch in range(50): # Run 10 Epochs\n    total_loss = 0\n    \n    for batch_idx, (x_batch, y_batch) in enumerate(train_loader):\n        # Move to GPU\n        x_batch = x_batch.to(device)\n        y_batch = y_batch.to(device)\n        \n        optimizer.zero_grad()\n        \n        # Forward\n        out = model(x_batch, edge_index)\n        \n        # Shape Correction: [B, N, T] -> [B, T, N]\n        out = out.permute(0, 2, 1)\n        \n        # Loss\n        loss = criterion(out, y_batch, node_type_mask)\n        \n        # Backprop\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n    \n    # Print average loss for this epoch\n    avg_loss = total_loss / len(train_loader)\n    print(f\"Epoch {epoch+1}/50 | Train Loss: {avg_loss:.4f}\")\n\nprint(\"âœ… Training Complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-08T10:01:03.309063Z","iopub.status.idle":"2026-02-08T10:01:03.309373Z","shell.execute_reply.started":"2026-02-08T10:01:03.309231Z","shell.execute_reply":"2026-02-08T10:01:03.309251Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.optim as optim\n\n# ==========================================\n# CONFIGURATION\n# ==========================================\nEXTRA_EPOCHS = 50       \nTARGET_LOSS = 1.0       \nBEST_LOSS = float('inf') \n\n# Scheduler: Removed 'verbose=True' to fix the error\n# If loss doesn't improve for 3 epochs, it lowers LR by 50%\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer, mode='min', factor=0.5, patience=3\n)\n\nprint(f\"ğŸš€ Resuming training for {EXTRA_EPOCHS} epochs (Target: {TARGET_LOSS})...\")\n\nmodel.train()\n\nfor epoch in range(EXTRA_EPOCHS):\n    total_loss = 0\n    \n    for batch_idx, (x_batch, y_batch) in enumerate(train_loader):\n        # Move to GPU\n        x_batch = x_batch.to(device)\n        y_batch = y_batch.to(device)\n        \n        optimizer.zero_grad()\n        \n        # Forward\n        out = model(x_batch, edge_index)\n        out = out.permute(0, 2, 1) # [B, N, T] -> [B, T, N]\n        \n        # Loss\n        loss = criterion(out, y_batch, node_type_mask)\n        \n        # Backprop\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n    \n    # Calculate Average Loss\n    avg_loss = total_loss / len(train_loader)\n    \n    # Update Learning Rate Scheduler\n    scheduler.step(avg_loss)\n    \n    # Get current learning rate for display\n    current_lr = optimizer.param_groups[0]['lr']\n    print(f\"Epoch {epoch+1}/{EXTRA_EPOCHS} | Loss: {avg_loss:.4f} | LR: {current_lr:.6f}\")\n    \n    # --- CHECKPOINTING ---\n    if avg_loss < BEST_LOSS:\n        BEST_LOSS = avg_loss\n        torch.save(model.state_dict(), 'best_model.pth')\n        print(f\"   ğŸ’¾ New Best Model Saved! (Loss: {BEST_LOSS:.4f})\")\n        \n    # --- EARLY STOPPING ---\n    if avg_loss <= TARGET_LOSS:\n        print(f\"âœ… Target Reached! Stopping early at Epoch {epoch+1}\")\n        break\n\nprint(f\"ğŸ Training Finished. Best Loss Achieved: {BEST_LOSS:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-08T10:01:03.310329Z","iopub.status.idle":"2026-02-08T10:01:03.310660Z","shell.execute_reply.started":"2026-02-08T10:01:03.310489Z","shell.execute_reply":"2026-02-08T10:01:03.310507Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\n\n# ==========================================\n# 1. LOAD & PREPARE TEST DATA\n# ==========================================\nprint(\"â³ Loading Test Data...\")\nt1 = pd.read_csv('/kaggle/input/urbanfloodbench/test_1d_nodes_dynamic_all.csv')\nt2 = pd.read_csv('/kaggle/input/urbanfloodbench/test_2d_nodes_dynamic_all.csv')\n\n# --- A. Pivot Forcing Data (Rain/Flow) ---\nprint(\"   Processing Forcing Features...\")\nf1 = t1.pivot_table(index='timestep', columns='node_idx', values='inlet_flow').fillna(0)\n\nif 'rainfall' in t2.columns:\n    f2 = t2.pivot_table(index='timestep', columns='node_idx', values='rainfall').fillna(0)\nelse:\n    # Fallback if no rainfall column\n    f2 = pd.DataFrame(0, index=f1.index, columns=range(len(t2['node_idx'].unique())))\n\n# --- B. FIX: ALIGN TIMESTEPS (Crucial Step) ---\n# Find the maximum range of time covered by EITHER file\n# usually 2D rain file is the master clock\nmax_t = max(f1.index.max(), f2.index.max()) if not f2.empty else f1.index.max()\nfull_timeline = range(int(max_t) + 1)\n\n# Reindex both to ensure they have exactly the same rows (0 to max_t)\nprint(f\"   Aligning timelines: 1D had {len(f1)} steps, 2D had {len(f2)} steps -> Target: {len(full_timeline)}\")\nf1 = f1.reindex(full_timeline, fill_value=0)\nf2 = f2.reindex(full_timeline, fill_value=0)\n\n# --- C. ALIGN COLUMNS (Ensure all nodes are present) ---\n# Ensure f1 has columns 0..16 (or whatever num_1d is)\n# Ensure f2 has columns 0..3715 (or whatever num_2d is)\n# We use the counts from your graph build\nnum_1d = 17    # Hardcoded from your graph build log\nnum_2d = 3716  # Hardcoded from your graph build log\n\n# Reindex columns to guarantee order matches graph.x\nf1 = f1.reindex(columns=range(num_1d), fill_value=0)\nf2 = f2.reindex(columns=range(num_2d), fill_value=0)\n\n# --- D. MERGE ---\n# Now shapes are guaranteed to match: [Max_Time, Num_Nodes]\nforcing_matrix = np.hstack([f1.values, f2.values]) \nprint(f\"   Forcing Matrix Shape: {forcing_matrix.shape}\")\n\n# Initialize Water Level Matrix (Starts at 0)\nnum_timesteps, num_nodes = forcing_matrix.shape\nwater_matrix = np.zeros((num_timesteps, num_nodes))\n\n# Create Master Tensor\ntest_tensor = np.stack([water_matrix, forcing_matrix], axis=-1)\ntest_tensor = torch.FloatTensor(test_tensor).to(device)\n\nprint(f\"âœ… Inference Environment Ready. Tensor Shape: {test_tensor.shape}\")\n\n# ==========================================\n# 2. RUN AUTOREGRESSIVE LOOP\n# ==========================================\nmodel.eval()\nWINDOW_SIZE = 12\n\nprint(\"ğŸš€ Starting Prediction Loop...\")\n# Create a list to store predictions to convert to DataFrame later\npreds_list = []\n\nwith torch.no_grad():\n    for t in range(num_timesteps):\n        # 1. Construct Input Window\n        if t < WINDOW_SIZE:\n            padding = test_tensor[0:1].repeat(WINDOW_SIZE - (t + 1), 1, 1)\n            history = test_tensor[0 : t + 1]\n            input_window = torch.cat([padding, history], dim=0)\n        else:\n            input_window = test_tensor[t - WINDOW_SIZE + 1 : t + 1]\n        \n        input_window = input_window.unsqueeze(0) # [1, 12, N, 2]\n        \n        # 2. Predict\n        preds = model(input_window, edge_index) # [1, N, 12]\n        \n        # 3. Update State\n        next_val = preds[0, :, 0] # Take first step prediction [N]\n        \n        # Store for submission (Save only what we need to save memory)\n        # We need this 'next_val' to be the water level at time 't' (or t+1 depending on offset)\n        # Usually, if we input [t-11...t], model predicts [t+1]. \n        # But for loop continuity, let's assume we predict 't' given previous.\n        # Let's align: We predict for the CURRENT step 't' using 't-1' data.\n        \n        # Update Tensor for NEXT iteration\n        # Note: If we are at step t, we just predicted water level for step t (or t+1).\n        # Let's assume prediction is for t+1. \n        if t + 1 < num_timesteps:\n            test_tensor[t + 1, :, 0] = next_val\n            \n        # Extract 1D predictions for submission (Nodes 0 to 16)\n        # 2D predictions (Nodes 17+)\n        preds_list.append(next_val.cpu().numpy())\n\n        if t % 50 == 0: print(f\"   ... Simulated Step {t}/{num_timesteps}\")\n\n# ==========================================\n# 3. EXPORT SUBMISSION\n# ==========================================\nprint(\"ğŸ’¾ Saving Results...\")\n# Convert list of arrays to big array [Time, Nodes]\nall_preds = np.stack(preds_list)\n\n# Flatten to Long Format\nflat_data = []\nfor t in range(num_timesteps):\n    # Only iterate 1D nodes if 2D is not required, OR iterate all if needed.\n    # Competition usually asks for specific nodes. \n    # Let's load sample submission to map exactly what is needed.\n    pass \n\n# FASTER MAPPING:\n# Create a DataFrame from the numpy array directly\ndf_preds = pd.DataFrame(all_preds, columns=range(num_nodes))\ndf_preds['timestep'] = range(num_timesteps) # Add timestep index\n# Melt to long format\ndf_long = df_preds.melt(id_vars='timestep', var_name='graph_node_idx', value_name='water_level')\n\n# Map 'graph_node_idx' back to (node_type, node_id)\n# 1D: 0..16 -> Type 1, ID = idx\n# 2D: 17..End -> Type 0, ID = idx - 17\ndef get_node_info(idx):\n    if idx < num_1d: return 1, idx\n    else: return 0, idx - num_1d\n\n# Vectorized mapping is faster\ndf_long['node_type'] = np.where(df_long['graph_node_idx'] < num_1d, 1, 0)\ndf_long['node_id'] = np.where(df_long['graph_node_idx'] < num_1d, df_long['graph_node_idx'], df_long['graph_node_idx'] - num_1d)\n\n# Load Sample Sub\nsample_sub = pd.read_csv('/kaggle/input/urbanfloodbench/sample_submission.csv')\nmerge_keys = ['node_type', 'node_id', 'timestep']\n\n# If sample_sub missing timestep, generate it (assuming sorted blocks)\nif 'timestep' not in sample_sub.columns:\n    sample_sub['timestep'] = sample_sub.groupby(['model_id', 'event_id', 'node_type', 'node_id']).cumcount()\n\n# Merge\nfinal_df = sample_sub.drop(columns=['water_level']).merge(\n    df_long[['node_type', 'node_id', 'timestep', 'water_level']], \n    on=merge_keys, \n    how='left'\n)\n\nfinal_df['water_level'] = final_df['water_level'].fillna(0)\nfinal_df.to_csv('submission_gnn.csv', index=False)\n\nprint(\"âœ… 'submission_gnn.csv' generated successfully!\")\nprint(final_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-08T10:01:03.312138Z","iopub.status.idle":"2026-02-08T10:01:03.312388Z","shell.execute_reply.started":"2026-02-08T10:01:03.312261Z","shell.execute_reply":"2026-02-08T10:01:03.312273Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# 1. Load the sample submission to use as a strict template\nsample_sub_template = pd.read_csv('/kaggle/input/urbanfloodbench/sample_submission.csv')\n\n# 2. Identify the required columns (e.g., model_id, event_id, node_type, node_id, water_level)\nrequired_columns = sample_sub_template.columns.tolist()\n\n# 3. Filter your final_df to include ONLY those columns\n# This effectively deletes 'timestep' and 'graph_node_idx'\nfinal_submission = final_df[required_columns].copy()\n\n# 4. Final verification: Check for NaNs one last time\nfinal_submission['water_level'] = final_submission['water_level'].fillna(0)\n\n# 5. Save the file\nfinal_submission.to_csv('submission.csv', index=False)\n\nprint(f\"âœ… 'submission.csv' generated with columns: {final_submission.columns.tolist()}\")\nprint(f\"Sample row:\\n{final_submission.head(1)}\")\n\n# Safety check: Compare column count\nif len(final_submission.columns) == len(required_columns):\n    print(\"ğŸš€ Column count matches sample_submission exactly!\")\nelse:\n    print(\"âš ï¸ Column count mismatch. Please check the required columns list.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-08T10:01:03.313675Z","iopub.status.idle":"2026-02-08T10:01:03.314051Z","shell.execute_reply.started":"2026-02-08T10:01:03.313858Z","shell.execute_reply":"2026-02-08T10:01:03.313879Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Sample row:\\n{final_submission.head(30)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-08T10:01:03.315109Z","iopub.status.idle":"2026-02-08T10:01:03.315445Z","shell.execute_reply.started":"2026-02-08T10:01:03.315294Z","shell.execute_reply":"2026-02-08T10:01:03.315314Z"}},"outputs":[],"execution_count":null}]}