{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14406473,"sourceType":"datasetVersion","datasetId":9200983}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-07T06:26:41.865494Z","iopub.execute_input":"2026-02-07T06:26:41.865867Z","iopub.status.idle":"2026-02-07T06:26:41.900418Z","shell.execute_reply.started":"2026-02-07T06:26:41.865830Z","shell.execute_reply":"2026-02-07T06:26:41.899344Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/urbanmodeling/test_2d_edges_dynamic_all.csv\n/kaggle/input/urbanmodeling/test_2d_edges_static.csv\n/kaggle/input/urbanmodeling/timesteps.csv\n/kaggle/input/urbanmodeling/sample_submission.csv\n/kaggle/input/urbanmodeling/test_2d_nodes_dynamic_all.csv\n/kaggle/input/urbanmodeling/test_1d_edges_dynamic_all.csv\n/kaggle/input/urbanmodeling/1d2d_connections.csv\n/kaggle/input/urbanmodeling/1d_nodes_static.csv\n/kaggle/input/urbanmodeling/2d_nodes_dynamic_all.csv\n/kaggle/input/urbanmodeling/2d_edges_dynamic_all.csv\n/kaggle/input/urbanmodeling/test_timesteps.csv\n/kaggle/input/urbanmodeling/1d_edge_index.csv\n/kaggle/input/urbanmodeling/2d_edges_static.csv\n/kaggle/input/urbanmodeling/test_2d_edge_index.csv\n/kaggle/input/urbanmodeling/1d_edges_static.csv\n/kaggle/input/urbanmodeling/test_1d_edges_static.csv\n/kaggle/input/urbanmodeling/1d_edges_dynamic_all.csv\n/kaggle/input/urbanmodeling/sample_submission.parquet\n/kaggle/input/urbanmodeling/2d_edge_index.csv\n/kaggle/input/urbanmodeling/2d_nodes_static.csv\n/kaggle/input/urbanmodeling/test_2d_nodes_static.csv\n/kaggle/input/urbanmodeling/test_1d_nodes_static.csv\n/kaggle/input/urbanmodeling/test_1d2d_connections.csv\n/kaggle/input/urbanmodeling/1d_nodes_dynamic_all.csv\n/kaggle/input/urbanmodeling/test_1d_nodes_dynamic_all.csv\n/kaggle/input/urbanmodeling/test_1d_edge_index.csv\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import pandas as pd\nimport os\n\nDATA_DIR = \"/kaggle/input/urbanmodeling\"\n\ndef load_urban_bench_data():\n    files = os.listdir(DATA_DIR)\n    data = {}\n    \n    # 1. Structural & Timing Files (The \"Skeleton\")\n    core_meta = [\n        '1d2d_connections', '1d_edge_index', '2d_edge_index', \n        'timesteps', 'test_timesteps', 'sample_submission'\n    ]\n    \n    # 2. Training Data (The \"Body\")\n    train_files = [\n        '1d_nodes_static', '1d_nodes_dynamic_all',\n        '1d_edges_static', '1d_edges_dynamic_all',\n        '2d_nodes_static', '2d_nodes_dynamic_all',\n        '2d_edges_static', '2d_edges_dynamic_all'\n    ]\n    \n    # 3. Test Data (The \"Challenge\")\n    test_files = [f\"test_{name}\" for name in train_files] + ['test_1d_edge_index', 'test_2d_edge_index', 'test_1d2d_connections']\n\n    all_categories = core_meta + train_files + test_files\n    \n    for cat in all_categories:\n        filename = f\"{cat}.csv\"\n        # Check if it exists (some might be .parquet or have slightly different names)\n        if filename in files:\n            data[cat] = pd.read_csv(os.path.join(DATA_DIR, filename))\n            print(f\"‚úÖ Loaded: {filename}\")\n        elif f\"{cat}.parquet\" in files:\n            data[cat] = pd.read_parquet(os.path.join(DATA_DIR, f\"{cat}.parquet\"))\n            print(f\"‚úÖ Loaded: {cat}.parquet\")\n            \n    return data\n\nurban_data = urban_data = load_urban_bench_data()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T06:26:41.902386Z","iopub.execute_input":"2026-02-07T06:26:41.902724Z","iopub.status.idle":"2026-02-07T06:27:02.984003Z","shell.execute_reply.started":"2026-02-07T06:26:41.902696Z","shell.execute_reply":"2026-02-07T06:27:02.983044Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Loaded: 1d2d_connections.csv\n‚úÖ Loaded: 1d_edge_index.csv\n‚úÖ Loaded: 2d_edge_index.csv\n‚úÖ Loaded: timesteps.csv\n‚úÖ Loaded: test_timesteps.csv\n‚úÖ Loaded: sample_submission.csv\n‚úÖ Loaded: 1d_nodes_static.csv\n‚úÖ Loaded: 1d_nodes_dynamic_all.csv\n‚úÖ Loaded: 1d_edges_static.csv\n‚úÖ Loaded: 1d_edges_dynamic_all.csv\n‚úÖ Loaded: 2d_nodes_static.csv\n‚úÖ Loaded: 2d_nodes_dynamic_all.csv\n‚úÖ Loaded: 2d_edges_static.csv\n‚úÖ Loaded: 2d_edges_dynamic_all.csv\n‚úÖ Loaded: test_1d_nodes_static.csv\n‚úÖ Loaded: test_1d_nodes_dynamic_all.csv\n‚úÖ Loaded: test_1d_edges_static.csv\n‚úÖ Loaded: test_1d_edges_dynamic_all.csv\n‚úÖ Loaded: test_2d_nodes_static.csv\n‚úÖ Loaded: test_2d_nodes_dynamic_all.csv\n‚úÖ Loaded: test_2d_edges_static.csv\n‚úÖ Loaded: test_2d_edges_dynamic_all.csv\n‚úÖ Loaded: test_1d_edge_index.csv\n‚úÖ Loaded: test_2d_edge_index.csv\n‚úÖ Loaded: test_1d2d_connections.csv\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nedge_df = pd.read_csv(\"/kaggle/input/urbanmodeling/1d_edge_index.csv\")\ndynamic_df = pd.read_csv('/kaggle/input/urbanmodeling/1d_nodes_dynamic_all.csv')\n\ndef get_node_connections(df):\n    \"\"\"\n    Finds all nodes connected to a specific node.\n    \"\"\"\n    # Upstream: Where water comes from TO our node\n    upstream_map = edge_df.groupby('to_node')['from_node'].apply(list).to_dict()\n\n    water_matrix = dynamic_df.pivot(index='timestep', columns='node_idx', values='water_level')\n\n    upstream_avg_matrix = pd.DataFrame(index=water_matrix.index, columns=water_matrix.columns)\n    \n    # Downstream: Where water goes FROM our node\n    downstream_map = df.groupby('from_node')['to_node'].apply(list).to_dict()\n    \n    return upstream_map, downstream_map\n\n# Example Usage:\n# Let's say we are looking at Node #5 in the 1D pipe network\nup_neighbors, down_neighbors = get_node_connections(edge_df)\n\nprint(f\"Node 13 is fed by (Upstream): {up_neighbors.get(13,[])}\")\nprint(f\"Node 13 drains into (Downstream): {down_neighbors.get(13, [])}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T06:27:02.985303Z","iopub.execute_input":"2026-02-07T06:27:02.985689Z","iopub.status.idle":"2026-02-07T06:27:03.028518Z","shell.execute_reply.started":"2026-02-07T06:27:02.985633Z","shell.execute_reply":"2026-02-07T06:27:03.027403Z"}},"outputs":[{"name":"stdout","text":"Node 13 is fed by (Upstream): [3, 7]\nNode 13 drains into (Downstream): [12]\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nwater_matrix = dynamic_df.pivot(index='timestep', columns='node_idx', values='water_level')\n\nupstream_map = edge_df.groupby('to_node')['from_node'].apply(list).to_dict()\n\nupstream_avg_matrix = pd.DataFrame(index=water_matrix.index, columns=water_matrix.columns)\n\nfor node in water_matrix.columns:\n    parents = upstream_map.get(node, [])\n    if parents:\n        # Average the water levels of all nodes that flow INTO this node\n        upstream_avg_matrix[node] = water_matrix[parents].mean(axis=1)\n    else:\n        # No upstream nodes? Use the node's own level or 0\n        upstream_avg_matrix[node] = 0 \n\n# 4. Merge back into a training format\nupstream_features = upstream_avg_matrix.reset_index().melt(\n    id_vars='timestep', var_name='node_idx', value_name='up_water_avg'\n)\ntrain_df = pd.merge(dynamic_df, upstream_features, on=['timestep', 'node_idx'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T06:27:03.030577Z","iopub.execute_input":"2026-02-07T06:27:03.030896Z","iopub.status.idle":"2026-02-07T06:27:03.071561Z","shell.execute_reply.started":"2026-02-07T06:27:03.030870Z","shell.execute_reply":"2026-02-07T06:27:03.070627Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"import pandas as pd\n\n# 1. Load the definitions\nedge_1d = pd.read_csv('/kaggle/input/urbanmodeling/1d_edge_index.csv')\nedge_2d = pd.read_csv('/kaggle/input/urbanmodeling/2d_edge_index.csv') # NEW\n\n# 2. Load the dynamic data for both systems\n# You need 2D data because that's where the rain \"lands\" first\ndynamic_1d = pd.read_csv('/kaggle/input/urbanmodeling/1d_nodes_dynamic_all.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T06:27:03.072751Z","iopub.execute_input":"2026-02-07T06:27:03.073030Z","iopub.status.idle":"2026-02-07T06:27:03.087756Z","shell.execute_reply.started":"2026-02-07T06:27:03.073005Z","shell.execute_reply":"2026-02-07T06:27:03.086631Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"import pandas as pd\n\n# --- 1. Load the Missing Data ---\n# You need the connections and the 2D dynamic history\nconn_df = pd.read_csv('/kaggle/input/urbanmodeling/1d2d_connections.csv')\ndynamic_2d = pd.read_csv('/kaggle/input/urbanmodeling/2d_nodes_dynamic_all.csv')\n\n# --- 2. Map 1D Nodes to 2D Nodes ---\n# Create a dictionary: {node_1d : node_2d}\nmap_1to2 = conn_df.set_index('node_1d')['node_2d'].to_dict()\n\n# --- 3. Prepare the 2D Data for Merging ---\n# We select only the columns we need: timestep, node index, and water level\nsurface_features = dynamic_2d[['timestep', 'node_idx', 'water_level']].copy()\n\n# Rename columns so they match train_df structure for merging\n# 'node_idx' becomes 'connected_2d_id' so we can join on it\n# 'water_level' becomes 'surface_water_level' so it doesn't clash with the 1D target\nsurface_features.columns = ['timestep', 'connected_2d_id', 'surface_water_level']\n\n# --- 4. Merge into train_df ---\n# First, tell train_df which 2D node it is connected to\ntrain_df['connected_2d_id'] = train_df['node_idx'].map(map_1to2)\n\n# Now, perform a LEFT JOIN. \n# We match rows based on 'timestep' and the 'connected_2d_id'.\nprint(\"Merging surface features... this might take a moment.\")\ntrain_df = pd.merge(train_df, surface_features, on=['timestep', 'connected_2d_id'], how='left')\n\n# Fill NaNs with 0 (For 1D nodes that don't have a surface connection)\ntrain_df['surface_water_level'] = train_df['surface_water_level'].fillna(0)\n\nprint(\"‚úÖ 'surface_water_level' successfully added to train_df!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T06:27:03.089096Z","iopub.execute_input":"2026-02-07T06:27:03.089382Z","iopub.status.idle":"2026-02-07T06:27:03.329934Z","shell.execute_reply.started":"2026-02-07T06:27:03.089355Z","shell.execute_reply":"2026-02-07T06:27:03.328840Z"}},"outputs":[{"name":"stdout","text":"Merging surface features... this might take a moment.\n‚úÖ 'surface_water_level' successfully added to train_df!\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"from xgboost import XGBRegressor\n\n# Create a 'Lag' (The level from the previous timestep)\ntrain_df['prev_water_level'] = train_df.groupby('node_idx')['water_level'].shift(1)\n\n# Drop timestep 0 because it has no 'previous' data\ntrain_ready = train_df.dropna(subset=['prev_water_level'])\n\n# Define Features and Target\n# We use the previous state and the current 'inlet_flow' to predict current level\nX = train_ready[['prev_water_level', 'up_water_avg', 'inlet_flow' , 'surface_water_level']]\ny = train_ready['water_level']\n\nmodel = XGBRegressor(n_estimators=100)\nmodel.fit(X, y)\n\nprint(\"Baseline Model Trained!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T06:27:03.331180Z","iopub.execute_input":"2026-02-07T06:27:03.331483Z","iopub.status.idle":"2026-02-07T06:27:04.647991Z","shell.execute_reply.started":"2026-02-07T06:27:03.331450Z","shell.execute_reply":"2026-02-07T06:27:04.646676Z"}},"outputs":[{"name":"stdout","text":"Baseline Model Trained!\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"import pandas as pd\n\nstatic_1d = pd.read_csv('/kaggle/input/urbanmodeling/1d_nodes_static.csv')\ntest_dynamic = pd.read_csv('/kaggle/input/urbanmodeling/test_1d_nodes_dynamic_all.csv')\n\ndef prepare_initial_state(test_df, static_df):\n    t0_data = test_df[test_df['timestep'] == 0].copy()\n\n    initial_state = t0_data.merge(static_df, on='node_idx', how='left')\n\n    print(f\"Initial State prepared for {len(initial_state)} nodes.\")\n    return initial_state\n\nstart_df = prepare_initial_state(test_dynamic, static_1d)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T06:27:04.648837Z","iopub.execute_input":"2026-02-07T06:27:04.649230Z","iopub.status.idle":"2026-02-07T06:27:04.666934Z","shell.execute_reply.started":"2026-02-07T06:27:04.649202Z","shell.execute_reply":"2026-02-07T06:27:04.665669Z"}},"outputs":[{"name":"stdout","text":"Initial State prepared for 17 nodes.\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"from xgboost import XGBRegressor\n \n# 'max_depth=6' helps prevent the model from memorizing the noise in the data.\nmy_trained_xgboost = XGBRegressor(\n    n_estimators=100,\n    learning_rate=0.1,\n    max_depth=6,\n    random_state=42,\n    objective='reg:squarederror' # minimize the mean squared error\n)\n\nprint(\"üèóÔ∏è Training the XGBoost model...\")\nmy_trained_xgboost.fit(X, y)\n\nprint(\"‚úÖ Model is now defined as 'my_trained_xgboost' and ready for the loop!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T06:27:04.668385Z","iopub.execute_input":"2026-02-07T06:27:04.668783Z","iopub.status.idle":"2026-02-07T06:27:04.861317Z","shell.execute_reply.started":"2026-02-07T06:27:04.668745Z","shell.execute_reply":"2026-02-07T06:27:04.860258Z"}},"outputs":[{"name":"stdout","text":"üèóÔ∏è Training the XGBoost model...\n‚úÖ Model is now defined as 'my_trained_xgboost' and ready for the loop!\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom xgboost import XGBRegressor\n\ndef run_aligned_prediction_loop(model, initial_state_df, test_timesteps, upstream_map, test_2d_df, map_1to2):\n    \"\"\"\n    Runs the autoregressive loop including 1D-2D coupling.\n    \"\"\"\n    current_state = initial_state_df.copy()\n    all_step_outputs = []\n\n    print(\"‚è≥ Indexing 2D data for lookup...\")\n    lookup_col = 'rainfall' if 'rainfall' in test_2d_df.columns else 'water_level'\n    test_2d_indexed = test_2d_df.set_index(['timestep', 'node_idx'])[lookup_col]\n\n    print(f\"‚úÖ Starting Prediction Loop using 2D feature: {lookup_col}\")\n    \n    # The loop 'walks' through time\n    for t in test_timesteps:\n        # 1. Update Spatial Features (Upstream Average)\n        water_map = current_state.set_index('node_idx')['water_level'].to_dict()\n        \n        def calc_up(n_idx):\n            parents = upstream_map.get(n_idx, [])\n            vals = [water_map.get(p, 0) for p in parents]\n            return np.mean(vals) if vals else 0\n\n        current_state['up_water_avg'] = current_state['node_idx'].apply(calc_up)\n\n        # 2. Update Coupled Features (Surface Water)\n        current_state['connected_2d_id'] = current_state['node_idx'].map(map_1to2)\n\n        def get_surface_val(row):\n            if pd.isna(row['connected_2d_id']):\n                return 0\n            try:\n                # We use 't' because 1D reacts to current surface rain/water\n                return test_2d_indexed.loc[(t, int(row['connected_2d_id']))]\n            except KeyError:\n                return 0\n        \n        current_state['surface_water_level'] = current_state.apply(get_surface_val, axis=1)\n\n        # 3. Prepare Features for Model (X)\n        features = ['water_level', 'up_water_avg', 'inlet_flow', 'surface_water_level']\n        X_test = current_state[features].copy()\n\n        X_test.columns = ['prev_water_level', 'up_water_avg', 'inlet_flow', 'surface_water_level'] \n\n        # 4. Predict T+1\n        predicted_levels = model.predict(X_test)\n\n        # 5. Update the state for the NEXT iteration\n        current_state['water_level'] = predicted_levels\n        current_state['timestep'] = t\n        \n        # 6. Store result\n        all_step_outputs.append(current_state.copy())\n\n    return pd.concat(all_step_outputs)\n\ndef export_to_kaggle(prediction_results_df, sample_sub_path='/kaggle/input/urbanmodeling/sample_submission.csv'):\n    sample_sub = pd.read_csv(sample_sub_path)\n    \n    merge_keys = ['model_id', 'event_id', 'node_type', 'node_id']\n    \n    final_df = sample_sub.drop(columns=['water_level']).merge(\n        prediction_results_df[merge_keys + ['water_level']], \n        on=merge_keys, \n        how='left'\n    )\n    \n    final_df = final_df.fillna(0) \n    final_df.to_csv('submission.csv', index=False)\n    print(\"‚úÖ submission.csv created with correct column alignment!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T06:27:04.863914Z","iopub.execute_input":"2026-02-07T06:27:04.864780Z","iopub.status.idle":"2026-02-07T06:27:04.876831Z","shell.execute_reply.started":"2026-02-07T06:27:04.864745Z","shell.execute_reply":"2026-02-07T06:27:04.876076Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"import pandas as pd\nconn_1d2d = pd.read_csv('/kaggle/input/urbanmodeling/1d2d_connections.csv') # NEW\n\ndef build_coupling_maps(connection_df):\n    \"\"\"\n    Creates dictionaries to instantly find the paired node.\n    \"\"\"\n    # Map 1D Node ID -> 2D Node ID (To find surface water entering the pipe)\n    # 1D nodes receive water from 2D nodes via these connections \n    map_1d_to_2d = connection_df.set_index('node_1d')['node_2d'].to_dict()\n    \n    # Map 2D Node ID -> 1D Node ID (To find where surface water drains to)\n    map_2d_to_1d = connection_df.set_index('node_2d')['node_1d'].to_dict()\n    \n    return map_1d_to_2d, map_2d_to_1d\n\n# Execute\nmap_1to2, map_2to1 = build_coupling_maps(conn_1d2d)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T06:27:04.878603Z","iopub.execute_input":"2026-02-07T06:27:04.879010Z","iopub.status.idle":"2026-02-07T06:27:04.905187Z","shell.execute_reply.started":"2026-02-07T06:27:04.878978Z","shell.execute_reply":"2026-02-07T06:27:04.903963Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# 1. Load the extra 2D Test data\ntest_2d_dynamic = pd.read_csv('/kaggle/input/urbanmodeling/test_2d_nodes_dynamic_all.csv')\n\n# 2. Define the Mapping (from previous steps)\nmap_1to2, _ = build_coupling_maps(pd.read_csv('/kaggle/input/urbanmodeling/1d2d_connections.csv'))\n\n# 3. Run the loop\nresults_df = run_aligned_prediction_loop(\n    model=my_trained_xgboost,\n    initial_state_df=start_df,     # Your prepared t=0 data\n    test_timesteps=range(1, 100),  # Adjust range based on actual test set length\n    upstream_map=upstream_map,     # Your 1D edge map\n    test_2d_df=test_2d_dynamic,    # The 2D Data source\n    map_1to2=map_1to2              # The coupling map\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T06:27:04.906578Z","iopub.execute_input":"2026-02-07T06:27:04.907618Z","iopub.status.idle":"2026-02-07T06:27:06.291456Z","shell.execute_reply.started":"2026-02-07T06:27:04.907586Z","shell.execute_reply":"2026-02-07T06:27:06.290424Z"}},"outputs":[{"name":"stdout","text":"‚è≥ Indexing 2D data for lookup...\n‚úÖ Starting Prediction Loop using 2D feature: rainfall\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"print(\"Cols in train_df:\", train_df.columns.tolist())\nif 'train_ready' in locals():\n    print(\"Cols in train_ready:\", train_ready.columns.tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T06:27:06.292525Z","iopub.execute_input":"2026-02-07T06:27:06.293143Z","iopub.status.idle":"2026-02-07T06:27:06.298752Z","shell.execute_reply.started":"2026-02-07T06:27:06.293113Z","shell.execute_reply":"2026-02-07T06:27:06.297912Z"}},"outputs":[{"name":"stdout","text":"Cols in train_df: ['timestep', 'node_idx', 'water_level', 'inlet_flow', 'up_water_avg', 'connected_2d_id', 'surface_water_level', 'prev_water_level']\nCols in train_ready: ['timestep', 'node_idx', 'water_level', 'inlet_flow', 'up_water_avg', 'connected_2d_id', 'surface_water_level', 'prev_water_level']\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"def get_coupled_features(current_1d_df, current_2d_df, map_1to2):\n    \"\"\"\n    Enriches the 1D data with the status of the connected 2D surface node.\n    \"\"\"\n    # 1. Create a lookup for current 2D water levels\n    # Key: 2D Node ID, Value: Current Water Level\n    surface_water_map = current_2d_df.set_index('node_idx')['water_level'].to_dict()\n    \n    # 2. Find the paired 2D node for every 1D node\n    # Note: Not all 1D nodes have a surface connection, so we map and fillna\n    current_1d_df['connected_2d_id'] = current_1d_df['node_idx'].map(map_1to2)\n    \n    # 3. Retrieve the water level of that paired 2D node\n    current_1d_df['surface_water_level'] = current_1d_df['connected_2d_id'].map(surface_water_map).fillna(0)\n    \n    return current_1d_df\n\n# --- Usage in your Training/Prediction Loop ---\n# Apply the coupling logic\ntrain_df_coupled = get_coupled_features(train_df, train_ready, map_1to2)\n\n#\"Vertical\" awareness (Surface -> Pipe)\nX = train_df_coupled[['prev_water_level', 'up_water_avg', 'inlet_flow', 'surface_water_level']]\n\nX = train_ready[['prev_water_level', 'up_water_avg', 'inlet_flow', 'surface_water_level']]\ny = train_ready['water_level']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T06:27:06.299873Z","iopub.execute_input":"2026-02-07T06:27:06.300151Z","iopub.status.idle":"2026-02-07T06:27:06.323190Z","shell.execute_reply.started":"2026-02-07T06:27:06.300116Z","shell.execute_reply":"2026-02-07T06:27:06.322127Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"from xgboost import XGBRegressor\n\n# 1. Define the model\n# use 'hist' for speed, and slightly deeper trees to capture the complex 1D-2D interaction\nmodel = XGBRegressor(\n    n_estimators=150,\n    learning_rate=0.05,\n    max_depth=8,\n    random_state=42,\n    objective='reg:squarederror',\n    tree_method='hist'\n)\n\n# 2. Train\nprint(\"üöÄ Training XGBoost on 4 coupled features...\")\nmodel.fit(X, y)\n\nprint(\"‚úÖ Model Trained successfully!\")\nprint(\"Feature Importances:\", dict(zip(X.columns, model.feature_importances_.round(4))))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T06:27:06.324360Z","iopub.execute_input":"2026-02-07T06:27:06.324737Z","iopub.status.idle":"2026-02-07T06:27:06.524698Z","shell.execute_reply.started":"2026-02-07T06:27:06.324699Z","shell.execute_reply":"2026-02-07T06:27:06.524067Z"}},"outputs":[{"name":"stdout","text":"üöÄ Training XGBoost on 4 coupled features...\n‚úÖ Model Trained successfully!\nFeature Importances: {'prev_water_level': np.float32(0.9994), 'up_water_avg': np.float32(0.0002), 'inlet_flow': np.float32(0.0002), 'surface_water_level': np.float32(1e-04)}\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"def run_coupled_prediction_loop(model, initial_state_df, test_timesteps, \n                                upstream_map, test_1d_dynamic_df, test_2d_dynamic_df, map_1to2):\n    \"\"\"\n    Runs autoregressive prediction matching the 4-feature training structure.\n    \"\"\"\n    current_state = initial_state_df.copy()\n    all_step_outputs = []\n    \n    # --- PRE-PROCESSING OPTIMIZATION ---\n    print(\"‚è≥ Optimizing lookup tables...\")\n    \n    # 1. Index 2D Surface Data (Time + ID -> Water Level/Rainfall)\n    # Note: If test 2d 'water_level' is NaN, we might fallback to 'rainfall' or 0\n    lookup_col = 'rainfall' if 'rainfall' in test_2d_dynamic_df.columns else 'water_level'\n    surface_source = test_2d_dynamic_df.set_index(['timestep', 'node_idx'])[lookup_col]\n    \n    # 2. Index 1D Inlet Flow (Time + ID -> Inlet Flow)\n    # We need to grab the 'inlet_flow' provided in the test file for future steps\n    inlet_source = test_1d_dynamic_df.set_index(['timestep', 'node_idx'])['inlet_flow']\n\n    print(f\"‚úÖ Loop initialized. Predicting {len(test_timesteps)} timesteps...\")\n\n    for t in test_timesteps:\n        # --- FEATURE 1: PREV_WATER_LEVEL ---\n        # The 'water_level' column in current_state IS the 'prev_water_level' for the next step\n        # We rename it later just before prediction\n        \n        # --- FEATURE 2: UP_WATER_AVG (Spatial 1D) ---\n        water_map = current_state.set_index('node_idx')['water_level'].to_dict()\n        \n        def calc_up(n_idx):\n            parents = upstream_map.get(n_idx, [])\n            vals = [water_map.get(p, 0) for p in parents]\n            return np.mean(vals) if vals else 0\n            \n        current_state['up_water_avg'] = current_state['node_idx'].apply(calc_up)\n        \n        # --- FEATURE 3: SURFACE_WATER_LEVEL (Coupled 2D) ---\n        current_state['connected_2d_id'] = current_state['node_idx'].map(map_1to2)\n        \n        def get_surface(row):\n            if pd.isna(row['connected_2d_id']): return 0\n            # Look up the value at time 't'\n            try: return surface_source.loc[(t, int(row['connected_2d_id']))]\n            except KeyError: return 0\n            \n        current_state['surface_water_level'] = current_state.apply(get_surface, axis=1)\n        \n        # --- FEATURE 4: INLET_FLOW (Forcing) ---\n        # We must look up the inlet_flow for the current time 't' from the test file\n        def get_inlet(row):\n            try: return inlet_source.loc[(t, int(row['node_idx']))]\n            except KeyError: return 0\n            \n        current_state['inlet_flow'] = current_state.apply(get_inlet, axis=1)\n\n        # --- PREPARE INPUT X ---\n        # Select the 4 columns. \n        # CRITICAL: 'water_level' represents the state at t-1, so it becomes 'prev_water_level'\n        features_to_select = ['water_level', 'up_water_avg', 'inlet_flow', 'surface_water_level']\n        X_test = current_state[features_to_select].copy()\n        \n        # Rename to match training exactly\n        X_test.columns = ['prev_water_level', 'up_water_avg', 'inlet_flow', 'surface_water_level']\n        \n        # --- PREDICT ---\n        predicted_levels = model.predict(X_test)\n        \n        # --- UPDATE STATE ---\n        current_state['water_level'] = predicted_levels\n        current_state['timestep'] = t\n        \n        all_step_outputs.append(current_state.copy())\n        \n        if t % 10 == 0: print(f\"   ...Step {t} done\")\n\n    return pd.concat(all_step_outputs)\n\n# --- EXECUTION ---\n# 1. Load Test Data\nprint(\"Loading Test Data...\")\ntest_dynamic_1d = pd.read_csv('/kaggle/input/urbanmodeling/test_1d_nodes_dynamic_all.csv')\ntest_dynamic_2d = pd.read_csv('/kaggle/input/urbanmodeling/test_2d_nodes_dynamic_all.csv')\nstatic_1d = pd.read_csv('/kaggle/input/urbanmodeling/1d_nodes_static.csv')\n\n# 2. Prepare Initialization (t=0)\nstart_df = test_dynamic_1d[test_dynamic_1d['timestep'] == 0].copy()\n# We don't need to merge static features unless we used them in training. \n# Our current model only uses Dynamic features, so this is fine.\n\n# 3. Run Loop\nresults_df = run_coupled_prediction_loop(\n    model=model,\n    initial_state_df=start_df,\n    test_timesteps=range(1, 101), # Adjust range as needed! Usually 1 to max_test_step\n    upstream_map=upstream_map,\n    test_1d_dynamic_df=test_dynamic_1d,\n    test_2d_dynamic_df=test_dynamic_2d,\n    map_1to2=map_1to2\n)\n\n# 4. Export\ndef export_submission(preds_df):\n    sample = pd.read_csv('/kaggle/input/urbanmodeling/sample_submission.csv')\n    keys = ['model_id', 'event_id', 'node_type', 'node_id', 'timestep'] \n    \n    # Note: sample_submission usually doesn't have 'timestep' column explicitly \n    # but relies on row_id. If specific logic is needed, we merge on IDs.\n    # Safe Merge Logic:\n    merge_keys = ['model_id', 'event_id', 'node_type', 'node_id']\n    \n    # We must be careful to match row-by-row. \n    # Usually, we ensure preds_df is sorted exactly like sample_submission.\n    # For now, let's just save the raw predictions to check them.\n    preds_df.to_csv('my_raw_predictions.csv', index=False)\n    print(\"‚úÖ Predictions generated! Check 'my_raw_predictions.csv'.\")\n\nexport_submission(results_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T06:27:06.526230Z","iopub.execute_input":"2026-02-07T06:27:06.526544Z","iopub.status.idle":"2026-02-07T06:27:23.342948Z","shell.execute_reply.started":"2026-02-07T06:27:06.526512Z","shell.execute_reply":"2026-02-07T06:27:23.341727Z"}},"outputs":[{"name":"stdout","text":"Loading Test Data...\n‚è≥ Optimizing lookup tables...\n‚úÖ Loop initialized. Predicting 100 timesteps...\n   ...Step 10 done\n   ...Step 20 done\n   ...Step 30 done\n   ...Step 40 done\n   ...Step 50 done\n   ...Step 60 done\n   ...Step 70 done\n   ...Step 80 done\n   ...Step 90 done\n   ...Step 100 done\n‚úÖ Predictions generated! Check 'my_raw_predictions.csv'.\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"import pandas as pd\n\n# 1. Load your raw predictions\nresults_df = pd.read_csv('my_raw_predictions.csv')\n\n# --- FIX START: PREPARE COLUMNS FOR MERGE ---\n\n# A. Rename 'node_idx' to 'node_id' to match submission format\nresults_df = results_df.rename(columns={'node_idx': 'node_id'})\n\n# B. Add the missing Metadata Columns\n# NOTE: These values must match the specific test file you used. \n# Based on the Kaggle sample data structure, we assume Model 1, Event 1, Node Type 1 (1D).\nresults_df['node_type'] = 1   # 1 = 1D Nodes\nresults_df['model_id'] = 1    # Assumption: You tested Model 1\nresults_df['event_id'] = 1    # Assumption: You tested Event 1\n\n# Check the head to ensure keys are present\nprint(\"Updated Results Head:\")\nprint(results_df[['model_id', 'event_id', 'node_type', 'node_id', 'water_level']].head())\n\n# --- FIX END ---\n\n# 2. Load Sample Submission\nsample_sub = pd.read_csv('/kaggle/input/urbanmodeling/sample_submission.csv')\nmerge_keys = ['model_id', 'event_id', 'node_type', 'node_id']\n\n# 3. Merge Predictions\n# We perform a LEFT merge on sample_sub to keep the submission structure intact.\n# We also merge on 'timestep' if possible, but sample_submission often relies on row order.\n# If 'timestep' is NOT in sample_submission, we rely on the specific keys + row mapping.\n# However, merging on just IDs without timestep allows us to update the specific rows.\n\n# CRITICAL: The sample submission has multiple rows (timesteps) for the same node.\n# We need to ensure we map the correct timestep.\n# If sample_submission lacks 'timestep', we must assume the user's predictions \n# cover the same time range and merge carefully. \n# A safer way if timestep is missing in sample_sub is to rely on 'row_id' or sorted order.\n\n# Let's check if timestep exists in sample_sub\nif 'timestep' not in sample_sub.columns:\n    # Create a dummy timestep for sample_sub to allow accurate merging\n    # assuming it's sorted by time within each group\n    sample_sub['timestep'] = sample_sub.groupby(merge_keys).cumcount() + 1\n\n# Now we merge using ALL keys including timestep\nfinal_df = sample_sub.merge(\n    results_df[['model_id', 'event_id', 'node_type', 'node_id', 'timestep', 'water_level']], \n    on=['model_id', 'event_id', 'node_type', 'node_id', 'timestep'], \n    how='left',\n    suffixes=('_sample', '')\n)\n\n# 4. Fill Values\n# If the merge found a match, use the new 'water_level'. If not (e.g., 2D nodes), keep 'water_level_sample' (usually 0)\nfinal_df['water_level'] = final_df['water_level'].fillna(final_df['water_level_sample'])\n\n# 5. Clean up columns\nfinal_df = final_df.drop(columns=['water_level_sample', 'timestep']) # Remove helper columns\n\n# 6. Final Safety Fill (Fill remaining NaNs with 0)\nfinal_df['water_level'] = final_df['water_level'].fillna(0)\n\n# 7. Save\nfinal_df.to_csv('submission.csv', index=False)\n\nprint(\"‚úÖ 'submission.csv' generated successfully!\")\nprint(f\"Submission Shape: {final_df.shape}\")\nprint(final_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T06:59:23.106158Z","iopub.execute_input":"2026-02-07T06:59:23.108793Z","iopub.status.idle":"2026-02-07T07:01:54.219245Z","shell.execute_reply.started":"2026-02-07T06:59:23.108673Z","shell.execute_reply":"2026-02-07T07:01:54.218097Z"}},"outputs":[{"name":"stdout","text":"Updated Results Head:\n   model_id  event_id  node_type  node_id  water_level\n0         1         1          1        0    288.25357\n1         1         1          1        1    286.70358\n2         1         1          1        2    310.07175\n3         1         1          1        3    305.17590\n4         1         1          1        4    310.36792\n‚úÖ 'submission.csv' generated successfully!\nSubmission Shape: (50910192, 6)\n   row_id  model_id  event_id  node_type  node_id  water_level\n0       0         1         5          1        0          0.0\n1       1         1         5          1        0          0.0\n2       2         1         5          1        0          0.0\n3       3         1         5          1        0          0.0\n4       4         1         5          1        0          0.0\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"target_timesteps = range(1, 11) \n\n# 2. Run the loop (using the 'run_aligned_prediction_loop' function from previously)\nprint(\"üèÉ Starting the prediction loop...\")\nprediction_results = run_aligned_prediction_loop(\n    model=my_trained_xgboost, \n    initial_state_df=start_df, \n    test_timesteps=target_timesteps, \n    upstream_map=up_neighbors\n)\n\n# 3. Format and Export\nprint(\"üìù Formatting submission...\")\nexport_to_kaggle(prediction_results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T06:27:38.940195Z","iopub.status.idle":"2026-02-07T06:27:38.940520Z","shell.execute_reply.started":"2026-02-07T06:27:38.940365Z","shell.execute_reply":"2026-02-07T06:27:38.940383Z"}},"outputs":[],"execution_count":null}]}